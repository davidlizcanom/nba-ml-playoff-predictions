{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèÄ NBA Playoffs Simulator ‚Äî Model Calibration (v2)\n",
        "**Proyecto:** Simul√© los Playoffs NBA miles de veces‚Ä¶ y encontr√© un contender inesperado\n",
        "\n",
        "**Notebook 03:** Entrenamiento, validaci√≥n y calibraci√≥n del modelo XGBoost\n",
        "\n",
        "En este notebook:\n",
        "1. Seleccionamos los **features m√°s predictivos** (menos es m√°s)\n",
        "2. Entrenamos un **XGBoost optimizado** para datasets peque√±os\n",
        "3. Validamos con **Leave-One-Season-Out** para asegurar generalizaci√≥n\n",
        "4. Calibramos probabilidades y hacemos **backtest hist√≥rico**\n",
        "5. Exportamos el modelo listo para la simulaci√≥n Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# SETUP: Dependencias y carga de datos\n",
        "# ============================================================\n",
        "!pip install xgboost --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import (\n",
        "    LeaveOneGroupOut,\n",
        "    StratifiedKFold\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    brier_score_loss,\n",
        "    log_loss,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "print('‚úÖ Dependencias cargadas')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Montar Drive y cargar datos del Notebook 02\n",
        "# ============================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/nba-playoffs-simulator'\n",
        "DATA_DIR = f'{PROJECT_DIR}/data'\n",
        "\n",
        "# Cargar datasets\n",
        "df_training = pd.read_csv(f'{DATA_DIR}/training_matchups.csv')\n",
        "df_profiles = pd.read_csv(f'{DATA_DIR}/team_profiles_2026.csv')\n",
        "\n",
        "# Cargar lista completa de features\n",
        "with open(f'{DATA_DIR}/feature_columns.txt', 'r') as f:\n",
        "    ALL_FEATURE_COLS = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "# Temporadas hist√≥ricas\n",
        "HISTORICAL_SEASONS = [\n",
        "    '2015-16', '2016-17', '2017-18', '2018-19', '2019-20',\n",
        "    '2020-21', '2021-22', '2022-23', '2023-24', '2024-25'\n",
        "]\n",
        "\n",
        "print(f'‚úÖ Datos cargados:')\n",
        "print(f'  ‚Üí Training set:    {df_training.shape}')\n",
        "print(f'  ‚Üí Team profiles:   {df_profiles.shape}')\n",
        "print(f'  ‚Üí All features ({len(ALL_FEATURE_COLS)}): {ALL_FEATURE_COLS}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üî¨ Secci√≥n 1: Selecci√≥n de Features (menos es m√°s)\n",
        "\n",
        "Con solo ~150 series de entrenamiento, usar 14 features es demasiado.\n",
        "El modelo memoriza ruido en lugar de aprender patrones reales.\n",
        "\n",
        "**Regla pr√°ctica en ML:** con N muestras, no uses m√°s de ~N/10 a N/20 features.\n",
        "Con 150 muestras ‚Üí **5 a 8 features m√°ximo**.\n",
        "\n",
        "Vamos a seleccionar los que realmente importan usando:\n",
        "1. Correlaci√≥n con el target\n",
        "2. Mutual Information (captura relaciones no lineales)\n",
        "3. Conocimiento del dominio (lo que sabemos de basketball)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 1.1 ‚Äî Preparar datos completos\n",
        "# ============================================================\n",
        "available_features = [c for c in ALL_FEATURE_COLS if c in df_training.columns]\n",
        "\n",
        "X_all = df_training[available_features].fillna(0)\n",
        "y = df_training['team_a_won'].copy()\n",
        "groups = df_training['season'].copy()\n",
        "\n",
        "print(f'üìã Dataset: {X_all.shape[0]} series √ó {X_all.shape[1]} features')\n",
        "print(f'üìä Balance: {y.mean():.1%} favorito gana | {(1-y).mean():.1%} upset')\n",
        "print(f'üìä Temporadas: {groups.nunique()}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 1.2 ‚Äî Ranking de features por poder predictivo\n",
        "# ============================================================\n",
        "\n",
        "# Correlaci√≥n con el target\n",
        "correlations = X_all.corrwith(y).abs().sort_values(ascending=False)\n",
        "\n",
        "# Mutual Information (captura relaciones no lineales)\n",
        "mi_scores = mutual_info_classif(X_all, y, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=available_features).sort_values(ascending=False)\n",
        "\n",
        "# Combinar rankings\n",
        "ranking = pd.DataFrame({\n",
        "    'correlation': correlations,\n",
        "    'mutual_info': mi_series\n",
        "})\n",
        "\n",
        "# Normalizar y promediar\n",
        "for col in ['correlation', 'mutual_info']:\n",
        "    ranking[f'{col}_norm'] = ranking[col] / ranking[col].max()\n",
        "ranking['combined_score'] = (\n",
        "    0.5 * ranking['correlation_norm'] + 0.5 * ranking['mutual_info_norm']\n",
        ")\n",
        "ranking = ranking.sort_values('combined_score', ascending=False)\n",
        "\n",
        "print('üìä Ranking de features por poder predictivo:\\n')\n",
        "for feat, row in ranking.iterrows():\n",
        "    bar = '‚ñà' * int(row['combined_score'] * 30)\n",
        "    print(f'  {feat:<24} Corr: {row[\"correlation\"]:.3f}  '\n",
        "          f'MI: {row[\"mutual_info\"]:.3f}  '\n",
        "          f'Score: {row[\"combined_score\"]:.3f}  {bar}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 1.3 ‚Äî Seleccionar top features + validar con LOSO\n",
        "# ============================================================\n",
        "# Probamos con 3, 4, 5, 6, 7 features para encontrar el punto √≥ptimo\n",
        "\n",
        "logo = LeaveOneGroupOut()\n",
        "baseline_acc = y.mean()  # benchmark: siempre favorito\n",
        "\n",
        "print(f'üìä Benchmark (siempre favorito): {baseline_acc:.3f}\\n')\n",
        "print(f'üîç Buscando n√∫mero √≥ptimo de features...\\n')\n",
        "\n",
        "ranked_features = ranking.index.tolist()\n",
        "results_by_n = []\n",
        "\n",
        "for n_feat in range(2, min(len(ranked_features), 10) + 1):\n",
        "    selected = ranked_features[:n_feat]\n",
        "    X_sel = X_all[selected]\n",
        "\n",
        "    oof_preds = np.zeros(len(y))\n",
        "    oof_probs = np.zeros(len(y))\n",
        "\n",
        "    for train_idx, test_idx in logo.split(X_sel, y, groups):\n",
        "        temp_model = XGBClassifier(\n",
        "            n_estimators=50, max_depth=2, learning_rate=0.05,\n",
        "            subsample=0.7, colsample_bytree=0.8,\n",
        "            reg_alpha=2.0, reg_lambda=3.0, min_child_weight=5,\n",
        "            gamma=0.5, objective='binary:logistic',\n",
        "            eval_metric='logloss', random_state=42,\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "        temp_model.fit(X_sel.iloc[train_idx], y.iloc[train_idx])\n",
        "        oof_preds[test_idx] = temp_model.predict(X_sel.iloc[test_idx])\n",
        "        oof_probs[test_idx] = temp_model.predict_proba(X_sel.iloc[test_idx])[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y, oof_preds)\n",
        "    brier = brier_score_loss(y, oof_probs)\n",
        "    auc = roc_auc_score(y, oof_probs)\n",
        "\n",
        "    results_by_n.append({\n",
        "        'n_features': n_feat,\n",
        "        'features': selected,\n",
        "        'accuracy': acc,\n",
        "        'brier': brier,\n",
        "        'auc': auc,\n",
        "        'vs_baseline': acc - baseline_acc\n",
        "    })\n",
        "\n",
        "    marker = '‚úÖ' if acc > baseline_acc else '‚ùå'\n",
        "    print(f'  {marker} {n_feat} features ‚Üí Acc: {acc:.3f} ({acc - baseline_acc:+.3f} vs baseline)  '\n",
        "          f'Brier: {brier:.4f}  AUC: {auc:.3f}')\n",
        "\n",
        "df_feat_search = pd.DataFrame(results_by_n)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 1.4 ‚Äî Visualizar la b√∫squeda de features √≥ptimos\n",
        "# ============================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Accuracy vs n_features\n",
        "ax1 = axes[0]\n",
        "ax1.plot(df_feat_search['n_features'], df_feat_search['accuracy'],\n",
        "         'o-', color='#64B5F6', linewidth=2, markersize=8)\n",
        "ax1.axhline(y=baseline_acc, color='#FF5252', linestyle='--',\n",
        "            linewidth=2, label=f'Baseline ({baseline_acc:.0%})')\n",
        "ax1.set_xlabel('N√∫mero de features', fontsize=12)\n",
        "ax1.set_ylabel('Accuracy (LOSO)', fontsize=12)\n",
        "ax1.set_title('Accuracy vs Features', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "\n",
        "# Brier Score vs n_features\n",
        "ax2 = axes[1]\n",
        "ax2.plot(df_feat_search['n_features'], df_feat_search['brier'],\n",
        "         's-', color='#FFB74D', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('N√∫mero de features', fontsize=12)\n",
        "ax2.set_ylabel('Brier Score (menor = mejor)', fontsize=12)\n",
        "ax2.set_title('Calibraci√≥n vs Features', fontsize=13, fontweight='bold')\n",
        "\n",
        "# AUC vs n_features\n",
        "ax3 = axes[2]\n",
        "ax3.plot(df_feat_search['n_features'], df_feat_search['auc'],\n",
        "         'D-', color='#81C784', linewidth=2, markersize=8)\n",
        "ax3.axhline(y=0.5, color='#FF5252', linestyle='--',\n",
        "            linewidth=1, alpha=0.5, label='Random (0.5)')\n",
        "ax3.set_xlabel('N√∫mero de features', fontsize=12)\n",
        "ax3.set_ylabel('ROC AUC', fontsize=12)\n",
        "ax3.set_title('Discriminaci√≥n vs Features', fontsize=13, fontweight='bold')\n",
        "ax3.legend(fontsize=10)\n",
        "\n",
        "plt.suptitle('¬øCu√°ntos features necesita el modelo?',\n",
        "             fontsize=15, fontweight='bold', y=1.03)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_selection.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()\n",
        "\n",
        "print('\\nüí° Buscamos el punto donde: Accuracy > baseline, Brier m√°s bajo, AUC m√°s alto')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 1.5 ‚Äî Seleccionar la mejor configuraci√≥n\n",
        "# ============================================================\n",
        "\n",
        "# Criterio: mejor Brier Score entre las que superan el baseline en accuracy\n",
        "# Si ninguna supera el baseline en accuracy, elegimos la de mejor Brier\n",
        "beats_baseline = df_feat_search[df_feat_search['accuracy'] >= baseline_acc]\n",
        "\n",
        "if len(beats_baseline) > 0:\n",
        "    best_row = beats_baseline.loc[beats_baseline['brier'].idxmin()]\n",
        "    print('‚úÖ Encontramos configuraciones que superan el baseline.\\n')\n",
        "else:\n",
        "    # Si ninguna supera el baseline en accuracy pura, priorizamos calibraci√≥n\n",
        "    # Un modelo con buen Brier Score produce probabilidades confiables\n",
        "    # aunque no siempre acierte el ganador binario\n",
        "    best_row = df_feat_search.loc[df_feat_search['brier'].idxmin()]\n",
        "    print('‚ö†Ô∏è Ninguna configuraci√≥n supera el baseline en accuracy pura.')\n",
        "    print('   Pero eso est√° OK: para la simulaci√≥n Monte Carlo lo que importa')\n",
        "    print('   es la CALIBRACI√ìN de probabilidades, no la predicci√≥n binaria.')\n",
        "    print('   Un modelo con buen Brier Score produce simulaciones confiables.\\n')\n",
        "\n",
        "FEATURE_COLS = best_row['features']\n",
        "N_BEST = int(best_row['n_features'])\n",
        "\n",
        "print(f'üéØ Mejor configuraci√≥n: {N_BEST} features')\n",
        "print(f'   Accuracy: {best_row[\"accuracy\"]:.3f} (baseline: {baseline_acc:.3f})')\n",
        "print(f'   Brier:    {best_row[\"brier\"]:.4f}')\n",
        "print(f'   AUC:      {best_row[\"auc\"]:.3f}')\n",
        "print(f'\\nüìã Features seleccionados:')\n",
        "for i, feat in enumerate(FEATURE_COLS, 1):\n",
        "    print(f'   {i}. {feat}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ü§ñ Secci√≥n 2: Entrenar el XGBoost optimizado\n",
        "\n",
        "Ahora entrenamos con los features seleccionados y **hiperpar√°metros\n",
        "agresivamente regularizados** para un dataset peque√±o:\n",
        "\n",
        "| Par√°metro | Valor | Por qu√© |\n",
        "|---|---|---|\n",
        "| `n_estimators` | 50 | Pocos √°rboles ‚Üí menos memorizaci√≥n |\n",
        "| `max_depth` | 2 | √Årboles muy superficiales ‚Üí patrones simples |\n",
        "| `learning_rate` | 0.05 | Aprendizaje lento ‚Üí m√°s conservador |\n",
        "| `min_child_weight` | 5 | Cada hoja necesita ‚â•5 muestras |\n",
        "| `gamma` | 0.5 | Penaliza splits que no mejoren mucho |\n",
        "| `reg_alpha/lambda` | 2.0/3.0 | Regularizaci√≥n fuerte L1 + L2 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.1 ‚Äî Entrenar modelo final\n",
        "# ============================================================\n",
        "X = X_all[FEATURE_COLS].copy()\n",
        "\n",
        "model = XGBClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=2,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.7,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=2.0,\n",
        "    reg_lambda=3.0,\n",
        "    min_child_weight=5,\n",
        "    gamma=0.5,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "# Training performance (referencia)\n",
        "y_pred_train = model.predict(X)\n",
        "y_prob_train = model.predict_proba(X)[:, 1]\n",
        "\n",
        "print('‚úÖ Modelo entrenado')\n",
        "print(f'\\nüìä Performance en training (referencia):')\n",
        "print(f'  Accuracy:    {accuracy_score(y, y_pred_train):.3f}')\n",
        "print(f'  Brier Score: {brier_score_loss(y, y_prob_train):.4f}')\n",
        "print(f'  ROC AUC:     {roc_auc_score(y, y_prob_train):.3f}')\n",
        "print(f'\\nüí° Con m√°s regularizaci√≥n, el training accuracy baja pero la')\n",
        "print(f'   generalizaci√≥n mejora. Eso es exactamente lo que queremos.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.2 ‚Äî Feature Importance\n",
        "# ============================================================\n",
        "\n",
        "importance = pd.DataFrame({\n",
        "    'feature': FEATURE_COLS,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, max(5, len(FEATURE_COLS) * 0.6)))\n",
        "colors = plt.cm.YlOrRd(np.linspace(0.3, 1, len(importance)))\n",
        "ax.barh(importance['feature'], importance['importance'], color=colors)\n",
        "ax.set_xlabel('Importancia', fontsize=12)\n",
        "ax.set_title('¬øQu√© features importan m√°s para ganar una serie de playoffs?',\n",
        "             fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "top_feat = importance.iloc[-1]\n",
        "ax.annotate(f'‚Üê El m√°s predictivo',\n",
        "            xy=(top_feat['importance'], top_feat['feature']),\n",
        "            xytext=(top_feat['importance'] * 0.6, len(importance) - 1.5),\n",
        "            fontsize=11, color='#FFD700', fontweight='bold',\n",
        "            arrowprops=dict(arrowstyle='->', color='#FFD700', lw=1.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()\n",
        "\n",
        "print('\\nüìä Importancia por feature:')\n",
        "for _, row in importance.iloc[::-1].iterrows():\n",
        "    bar = '‚ñà' * int(row['importance'] * 50)\n",
        "    print(f'  {row[\"feature\"]:<24} {row[\"importance\"]:.3f}  {bar}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üß™ Secci√≥n 3: Validaci√≥n del Modelo\n",
        "\n",
        "**Leave-One-Season-Out (LOSO):** entrena con 9 temporadas, predice la restante.\n",
        "Repite para cada temporada. Simula el escenario real de predecir playoffs futuros.\n",
        "\n",
        "Ahora con menos features y m√°s regularizaci√≥n, el modelo deber√≠a generalizar mejor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1 ‚Äî Leave-One-Season-Out Cross Validation\n",
        "# ============================================================\n",
        "\n",
        "logo = LeaveOneGroupOut()\n",
        "\n",
        "oof_predictions = np.zeros(len(y))\n",
        "oof_probabilities = np.zeros(len(y))\n",
        "season_results = []\n",
        "\n",
        "print('üß™ Leave-One-Season-Out Cross Validation:\\n')\n",
        "\n",
        "for train_idx, test_idx in logo.split(X, y, groups):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    test_season = groups.iloc[test_idx].values[0]\n",
        "\n",
        "    temp_model = XGBClassifier(\n",
        "        n_estimators=50, max_depth=2, learning_rate=0.05,\n",
        "        subsample=0.7, colsample_bytree=0.8,\n",
        "        reg_alpha=2.0, reg_lambda=3.0, min_child_weight=5,\n",
        "        gamma=0.5, objective='binary:logistic',\n",
        "        eval_metric='logloss', random_state=42,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    temp_model.fit(X_train, y_train)\n",
        "\n",
        "    preds = temp_model.predict(X_test)\n",
        "    probs = temp_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    oof_predictions[test_idx] = preds\n",
        "    oof_probabilities[test_idx] = probs\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    n_series = len(y_test)\n",
        "    correct = int((preds == y_test.values).sum())\n",
        "\n",
        "    season_results.append({\n",
        "        'season': test_season,\n",
        "        'n_series': n_series,\n",
        "        'correct': correct,\n",
        "        'accuracy': acc\n",
        "    })\n",
        "\n",
        "    marker = '‚úÖ' if acc >= baseline_acc else '‚ö†Ô∏è'\n",
        "    print(f'  {marker} {test_season}: {correct}/{n_series} correctas ({acc:.0%})')\n",
        "\n",
        "# M√©tricas globales\n",
        "oof_acc = accuracy_score(y, oof_predictions)\n",
        "oof_brier = brier_score_loss(y, oof_probabilities)\n",
        "oof_auc = roc_auc_score(y, oof_probabilities)\n",
        "\n",
        "print(f'\\n{'='*50}')\n",
        "print(f'üìä M√©tricas globales (Out-of-Fold):')\n",
        "print(f'  Accuracy:    {oof_acc:.3f}  (baseline: {baseline_acc:.3f})')\n",
        "print(f'  Brier Score: {oof_brier:.4f}  (m√°s bajo = mejor calibraci√≥n)')\n",
        "print(f'  ROC AUC:     {oof_auc:.3f}  (> 0.5 = mejor que random)')\n",
        "print(f'\\nüìä Mejora vs baseline: {oof_acc - baseline_acc:+.3f}')\n",
        "\n",
        "if oof_acc >= baseline_acc:\n",
        "    print(f'\\n‚úÖ El modelo supera o iguala el baseline')\n",
        "else:\n",
        "    diff = baseline_acc - oof_acc\n",
        "    print(f'\\nüí° El modelo queda {diff:.3f} por debajo del baseline en accuracy.')\n",
        "    print(f'   Pero recordemos: para Monte Carlo, la CALIBRACI√ìN (Brier) importa')\n",
        "    print(f'   m√°s que la accuracy binaria. Un modelo que dice \"65% para A\"')\n",
        "    print(f'   es m√°s √∫til que uno que siempre dice \"100% para el favorito\".')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.2 ‚Äî Resultados por temporada (visual)\n",
        "# ============================================================\n",
        "df_season_results = pd.DataFrame(season_results)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "colors_bars = ['#00E676' if acc >= baseline_acc else '#64B5F6'\n",
        "               for acc in df_season_results['accuracy']]\n",
        "\n",
        "bars = ax.bar(df_season_results['season'], df_season_results['accuracy'],\n",
        "              color=colors_bars, edgecolor='white', linewidth=0.5)\n",
        "\n",
        "ax.axhline(y=baseline_acc, color='#FF5252', linestyle='--', linewidth=2,\n",
        "           label=f'Baseline: siempre favorito ({baseline_acc:.0%})')\n",
        "ax.axhline(y=oof_acc, color='#FFD700', linestyle='--', linewidth=2,\n",
        "           label=f'Modelo promedio ({oof_acc:.0%})')\n",
        "\n",
        "for bar, row in zip(bars, df_season_results.itertuples()):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "            f'{row.correct}/{row.n_series}',\n",
        "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "ax.set_ylim(0, 1.15)\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_title('Validaci√≥n Leave-One-Season-Out (modelo optimizado)',\n",
        "             fontsize=13, fontweight='bold', pad=15)\n",
        "ax.legend(fontsize=10, loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('validation_by_season.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìê Secci√≥n 4: Calibraci√≥n de Probabilidades\n",
        "\n",
        "Para la simulaci√≥n Monte Carlo, la calibraci√≥n es **m√°s importante que la accuracy**.\n",
        "\n",
        "¬øPor qu√©? Porque el Monte Carlo necesita probabilidades confiables:\n",
        "- Si el modelo dice \"60% para OKC\", eso debe significar que en situaciones similares,\n",
        "  OKC gana ~60% de las veces.\n",
        "- Un modelo que siempre dice \"favorito gana al 100%\" tiene alta accuracy pero\n",
        "  probabilidades terribles ‚Üí la simulaci√≥n ser√≠a in√∫til.\n",
        "\n",
        "El **Brier Score** mide exactamente esto: qu√© tan calibradas son las probabilidades."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 4.1 ‚Äî Calibration Plot\n",
        "# ============================================================\n",
        "\n",
        "n_bins = 5\n",
        "prob_true, prob_pred = calibration_curve(\n",
        "    y, oof_probabilities, n_bins=n_bins, strategy='uniform'\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Calibration curve\n",
        "ax1 = axes[0]\n",
        "ax1.plot([0, 1], [0, 1], 'w--', linewidth=1, alpha=0.5, label='Calibraci√≥n perfecta')\n",
        "ax1.plot(prob_pred, prob_true, 's-', color='#64B5F6', linewidth=2,\n",
        "         markersize=10, label='XGBoost')\n",
        "\n",
        "x_line = np.linspace(0, 1, 100)\n",
        "ax1.fill_between(x_line, x_line - 0.15, x_line + 0.15,\n",
        "                 alpha=0.1, color='#00E676', label='Zona aceptable (¬±15%)')\n",
        "\n",
        "ax1.set_xlabel('Probabilidad predicha', fontsize=12)\n",
        "ax1.set_ylabel('Frecuencia real de victoria', fontsize=12)\n",
        "ax1.set_title('Calibration Plot', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.set_xlim(0, 1)\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# Distribution of probabilities\n",
        "ax2 = axes[1]\n",
        "ax2.hist(oof_probabilities[y == 1], bins=12, alpha=0.7,\n",
        "         label='Favorito GAN√ì', color='#00E676')\n",
        "ax2.hist(oof_probabilities[y == 0], bins=12, alpha=0.7,\n",
        "         label='Favorito PERDI√ì', color='#FF5252')\n",
        "ax2.set_xlabel('Probabilidad predicha para favorito', fontsize=12)\n",
        "ax2.set_ylabel('Frecuencia', fontsize=12)\n",
        "ax2.set_title('Distribuci√≥n de probabilidades', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('calibration_plot.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nüìä Brier Score: {oof_brier:.4f}')\n",
        "print(f'   ‚Üí < 0.25: aceptable')\n",
        "print(f'   ‚Üí < 0.20: bueno')\n",
        "print(f'   ‚Üí < 0.15: muy bueno')\n",
        "print(f'\\nüìä Rango de probabilidades predichas:')\n",
        "print(f'   Min: {oof_probabilities.min():.3f}')\n",
        "print(f'   Max: {oof_probabilities.max():.3f}')\n",
        "print(f'   Std: {oof_probabilities.std():.3f}')\n",
        "print(f'\\nüí° Queremos que las probabilidades tengan RANGO (variaci√≥n).')\n",
        "print(f'   Si todas est√°n entre 0.65-0.75, el modelo no discrimina bien.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üèÜ Secci√≥n 5: Backtest Hist√≥rico\n",
        "\n",
        "Simulamos los playoffs de las **√∫ltimas 3 temporadas** para verificar\n",
        "que el modelo produce rankings razonables.\n",
        "\n",
        "Criterio de √©xito: el campe√≥n real debe aparecer en el **top 5**\n",
        "de probabilidades. Esto da credibilidad para el video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.1 ‚Äî Funciones de simulaci√≥n\n",
        "# ============================================================\n",
        "\n",
        "def simulate_series(prob_a_wins, n_games=7, rng=None):\n",
        "    \"\"\"\n",
        "    Simula una serie al mejor de 7.\n",
        "    Retorna True si Team A gana la serie.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    wins_a, wins_b = 0, 0\n",
        "    games_to_win = (n_games // 2) + 1\n",
        "\n",
        "    # Formato 2-2-1-1-1: juegos 1,2,5,7 en casa de A (mejor seed)\n",
        "    home_a_games = {1, 2, 5, 7}\n",
        "    home_boost = 0.03  # ~3% ventaja de local en NBA playoffs\n",
        "\n",
        "    game_num = 0\n",
        "    while wins_a < games_to_win and wins_b < games_to_win:\n",
        "        game_num += 1\n",
        "        p = prob_a_wins + (home_boost if game_num in home_a_games else -home_boost)\n",
        "        p = np.clip(p, 0.05, 0.95)\n",
        "\n",
        "        if rng.random() < p:\n",
        "            wins_a += 1\n",
        "        else:\n",
        "            wins_b += 1\n",
        "\n",
        "    return wins_a >= games_to_win\n",
        "\n",
        "\n",
        "def get_matchup_probability(team_a_stats, team_b_stats, feature_cols, model):\n",
        "    \"\"\"\n",
        "    Calcula la probabilidad de que Team A gane una serie.\n",
        "    \"\"\"\n",
        "    row = {}\n",
        "    for feat in feature_cols:\n",
        "        base_feat = feat.replace('_diff', '')\n",
        "        if base_feat in team_a_stats.index and base_feat in team_b_stats.index:\n",
        "            val_a = team_a_stats[base_feat]\n",
        "            val_b = team_b_stats[base_feat]\n",
        "            if pd.notna(val_a) and pd.notna(val_b):\n",
        "                if base_feat in ['DEF_RATING', 'TM_TOV_PCT']:\n",
        "                    row[feat] = val_b - val_a\n",
        "                else:\n",
        "                    row[feat] = val_a - val_b\n",
        "            else:\n",
        "                row[feat] = 0\n",
        "        elif feat == 'seed_diff':\n",
        "            seed_a = team_a_stats.get('SEED', team_a_stats.get('PlayoffRank', 4))\n",
        "            seed_b = team_b_stats.get('SEED', team_b_stats.get('PlayoffRank', 4))\n",
        "            row[feat] = seed_b - seed_a\n",
        "        else:\n",
        "            row[feat] = 0\n",
        "\n",
        "    X_matchup = pd.DataFrame([row])[feature_cols]\n",
        "    prob = model.predict_proba(X_matchup)[0][1]\n",
        "    return prob\n",
        "\n",
        "\n",
        "def simulate_playoffs(teams_east, teams_west, feature_cols, model,\n",
        "                      n_simulations=10000, seed=42):\n",
        "    \"\"\"\n",
        "    Simula el bracket completo de playoffs n veces.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    r1_matchups = [(0, 7), (3, 4), (2, 5), (1, 6)]\n",
        "\n",
        "    prob_cache = {}\n",
        "\n",
        "    def get_prob(team_a, team_b):\n",
        "        key = (team_a['TEAM_ID'], team_b['TEAM_ID'])\n",
        "        if key not in prob_cache:\n",
        "            prob_cache[key] = get_matchup_probability(\n",
        "                team_a, team_b, feature_cols, model\n",
        "            )\n",
        "        return prob_cache[key]\n",
        "\n",
        "    east_list = [teams_east.iloc[i] for i in range(len(teams_east))]\n",
        "    west_list = [teams_west.iloc[i] for i in range(len(teams_west))]\n",
        "\n",
        "    results = {team['TEAM_NAME']: {\n",
        "        'champion': 0, 'finals': 0, 'conf_finals': 0, 'conf_semis': 0\n",
        "    } for team in east_list + west_list}\n",
        "\n",
        "    finals_matchups = []\n",
        "\n",
        "    for sim in range(n_simulations):\n",
        "        conf_winners = {}\n",
        "\n",
        "        for conf_name, teams in [('East', east_list), ('West', west_list)]:\n",
        "            # Round 1\n",
        "            r1_winners = []\n",
        "            for seed_a, seed_b in r1_matchups:\n",
        "                prob = get_prob(teams[seed_a], teams[seed_b])\n",
        "                a_wins = simulate_series(prob, rng=rng)\n",
        "                winner = teams[seed_a] if a_wins else teams[seed_b]\n",
        "                r1_winners.append(winner)\n",
        "\n",
        "            # Conf Semis\n",
        "            for w in r1_winners:\n",
        "                results[w['TEAM_NAME']]['conf_semis'] += 1\n",
        "\n",
        "            r2_winners = []\n",
        "            for i in range(0, 4, 2):\n",
        "                a, b = r1_winners[i], r1_winners[i+1]\n",
        "                seed_a = a.get('SEED', a.get('PlayoffRank', 4))\n",
        "                seed_b = b.get('SEED', b.get('PlayoffRank', 4))\n",
        "                if seed_a <= seed_b:\n",
        "                    prob = get_prob(a, b)\n",
        "                    a_wins = simulate_series(prob, rng=rng)\n",
        "                    winner = a if a_wins else b\n",
        "                else:\n",
        "                    prob = get_prob(b, a)\n",
        "                    a_wins = simulate_series(prob, rng=rng)\n",
        "                    winner = b if a_wins else a\n",
        "                r2_winners.append(winner)\n",
        "\n",
        "            # Conf Finals\n",
        "            for w in r2_winners:\n",
        "                results[w['TEAM_NAME']]['conf_finals'] += 1\n",
        "\n",
        "            a, b = r2_winners[0], r2_winners[1]\n",
        "            seed_a = a.get('SEED', a.get('PlayoffRank', 4))\n",
        "            seed_b = b.get('SEED', b.get('PlayoffRank', 4))\n",
        "            if seed_a <= seed_b:\n",
        "                prob = get_prob(a, b)\n",
        "                a_wins = simulate_series(prob, rng=rng)\n",
        "                conf_winner = a if a_wins else b\n",
        "            else:\n",
        "                prob = get_prob(b, a)\n",
        "                a_wins = simulate_series(prob, rng=rng)\n",
        "                conf_winner = b if a_wins else a\n",
        "\n",
        "            conf_winners[conf_name] = conf_winner\n",
        "\n",
        "        # NBA Finals\n",
        "        east_champ = conf_winners['East']\n",
        "        west_champ = conf_winners['West']\n",
        "\n",
        "        results[east_champ['TEAM_NAME']]['finals'] += 1\n",
        "        results[west_champ['TEAM_NAME']]['finals'] += 1\n",
        "\n",
        "        finals_matchups.append((east_champ['TEAM_NAME'], west_champ['TEAM_NAME']))\n",
        "\n",
        "        if east_champ.get('NET_RATING', 0) >= west_champ.get('NET_RATING', 0):\n",
        "            prob = get_prob(east_champ, west_champ)\n",
        "            a_wins = simulate_series(prob, rng=rng)\n",
        "            champion = east_champ if a_wins else west_champ\n",
        "        else:\n",
        "            prob = get_prob(west_champ, east_champ)\n",
        "            a_wins = simulate_series(prob, rng=rng)\n",
        "            champion = west_champ if a_wins else east_champ\n",
        "\n",
        "        results[champion['TEAM_NAME']]['champion'] += 1\n",
        "\n",
        "    df_results = pd.DataFrame(results).T\n",
        "    for col in ['champion', 'finals', 'conf_finals', 'conf_semis']:\n",
        "        df_results[f'{col}_pct'] = (df_results[col] / n_simulations * 100).round(2)\n",
        "\n",
        "    df_results = df_results.sort_values('champion', ascending=False)\n",
        "\n",
        "    return df_results, finals_matchups\n",
        "\n",
        "\n",
        "print('‚úÖ Funciones de simulaci√≥n definidas')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.2 ‚Äî Ejecutar backtest\n",
        "# ============================================================\n",
        "\n",
        "BACKTEST_SEASONS = ['2022-23', '2023-24', '2024-25']\n",
        "KNOWN_CHAMPIONS = {\n",
        "    '2022-23': 'Denver Nuggets',\n",
        "    '2023-24': 'Boston Celtics',\n",
        "    '2024-25': None\n",
        "}\n",
        "\n",
        "# Buscar campe√≥n 2024-25\n",
        "finals_2025 = df_training[\n",
        "    (df_training['season'] == '2024-25') & (df_training['round'] == 4)\n",
        "]\n",
        "if len(finals_2025) > 0:\n",
        "    row = finals_2025.iloc[0]\n",
        "    KNOWN_CHAMPIONS['2024-25'] = (\n",
        "        row['team_a_abbr'] if row['team_a_won'] == 1 else row['team_b_abbr']\n",
        "    )\n",
        "\n",
        "print('üèÜ Campeones conocidos para backtest:')\n",
        "for s, c in KNOWN_CHAMPIONS.items():\n",
        "    print(f'  {s}: {c or \"(no encontrado)\"}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.3 ‚Äî Ejecutar backtest por temporada\n",
        "# ============================================================\n",
        "\n",
        "df_hist_stats = pd.read_csv(f'{DATA_DIR}/historical_team_stats.csv')\n",
        "df_hist_standings = pd.read_csv(f'{DATA_DIR}/historical_standings.csv')\n",
        "\n",
        "print('üß™ Ejecutando backtest...\\n')\n",
        "\n",
        "for season in BACKTEST_SEASONS:\n",
        "    print(f'‚îÅ‚îÅ‚îÅ {season} ‚îÅ‚îÅ‚îÅ')\n",
        "\n",
        "    # Entrenar sin esta temporada\n",
        "    mask_train = df_training['season'] != season\n",
        "    X_bt = df_training.loc[mask_train, FEATURE_COLS]\n",
        "    y_bt = df_training.loc[mask_train, 'team_a_won']\n",
        "\n",
        "    bt_model = XGBClassifier(\n",
        "        n_estimators=50, max_depth=2, learning_rate=0.05,\n",
        "        subsample=0.7, colsample_bytree=0.8,\n",
        "        reg_alpha=2.0, reg_lambda=3.0, min_child_weight=5,\n",
        "        gamma=0.5, objective='binary:logistic',\n",
        "        eval_metric='logloss', random_state=42,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    bt_model.fit(X_bt, y_bt)\n",
        "\n",
        "    # Perfiles de esa temporada\n",
        "    season_stats = df_hist_stats[df_hist_stats['SEASON'] == season].copy()\n",
        "    season_standings = df_hist_standings[df_hist_standings['SEASON'] == season].copy()\n",
        "\n",
        "    if season_stats.empty:\n",
        "        print(f'  ‚ö†Ô∏è Sin stats para {season}\\n')\n",
        "        continue\n",
        "\n",
        "    if 'TeamID' in season_standings.columns:\n",
        "        stand_merge = season_standings[['TeamID', 'Conference', 'PlayoffRank']].rename(\n",
        "            columns={'TeamID': 'TEAM_ID'}\n",
        "        )\n",
        "        season_stats = season_stats.merge(stand_merge, on='TEAM_ID', how='left')\n",
        "\n",
        "    if 'Conference' not in season_stats.columns or 'PlayoffRank' not in season_stats.columns:\n",
        "        print(f'  ‚ö†Ô∏è Sin Conference/Seed para {season}\\n')\n",
        "        continue\n",
        "\n",
        "    season_stats['SEED'] = season_stats['PlayoffRank'].astype(float)\n",
        "    east = season_stats[season_stats['Conference'] == 'East'].nsmallest(8, 'SEED')\n",
        "    west = season_stats[season_stats['Conference'] == 'West'].nsmallest(8, 'SEED')\n",
        "\n",
        "    if len(east) < 8 or len(west) < 8:\n",
        "        print(f'  ‚ö†Ô∏è Equipos insuficientes\\n')\n",
        "        continue\n",
        "\n",
        "    bt_results, _ = simulate_playoffs(\n",
        "        east, west, FEATURE_COLS, bt_model,\n",
        "        n_simulations=5000, seed=42\n",
        "    )\n",
        "\n",
        "    top5 = bt_results.head(5)\n",
        "    champion = KNOWN_CHAMPIONS.get(season, '?')\n",
        "\n",
        "    print(f'  Top 5 probabilidad de campeonato:')\n",
        "    for rank, (team, row) in enumerate(top5.iterrows(), 1):\n",
        "        marker = ' üèÜ' if champion and champion in team else ''\n",
        "        print(f'    {rank}. {team:<28} {row[\"champion_pct\"]:>6.1f}%{marker}')\n",
        "\n",
        "    if champion:\n",
        "        found = False\n",
        "        for pos, (team, _) in enumerate(bt_results.iterrows(), 1):\n",
        "            if champion in team:\n",
        "                if pos <= 5:\n",
        "                    print(f'  ‚úÖ {champion} est√° en el Top {pos}')\n",
        "                else:\n",
        "                    print(f'  ‚ö†Ô∏è {champion} est√° en posici√≥n #{pos}')\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            print(f'  ‚ùå {champion} no encontrado')\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üíæ Secci√≥n 6: Exportar modelo y configuraci√≥n\n",
        "\n",
        "Guardamos todo lo necesario para el notebook 04 (simulaci√≥n final)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 6.1 ‚Äî Guardar modelo, features y m√©tricas\n",
        "# ============================================================\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "MODELS_DIR = f'{PROJECT_DIR}/models'\n",
        "OUTPUTS_DIR = f'{PROJECT_DIR}/outputs'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
        "\n",
        "# Guardar modelo\n",
        "model_path = f'{MODELS_DIR}/xgb_playoff_model.pkl'\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "print(f'‚úÖ Modelo guardado: {model_path}')\n",
        "\n",
        "# Guardar features SELECCIONADOS (no todos)\n",
        "features_path = f'{MODELS_DIR}/feature_columns.txt'\n",
        "with open(features_path, 'w') as f:\n",
        "    f.write('\\n'.join(FEATURE_COLS))\n",
        "print(f'‚úÖ Features guardados ({len(FEATURE_COLS)}): {features_path}')\n",
        "\n",
        "# Guardar m√©tricas\n",
        "metrics = {\n",
        "    'oof_accuracy': round(oof_acc, 4),\n",
        "    'oof_brier_score': round(oof_brier, 4),\n",
        "    'oof_roc_auc': round(oof_auc, 4),\n",
        "    'baseline_accuracy': round(baseline_acc, 4),\n",
        "    'n_training_samples': int(len(y)),\n",
        "    'n_features': len(FEATURE_COLS),\n",
        "    'features_used': list(FEATURE_COLS),\n",
        "    'model_params': {\n",
        "        'n_estimators': 50, 'max_depth': 2, 'learning_rate': 0.05,\n",
        "        'subsample': 0.7, 'reg_alpha': 2.0, 'reg_lambda': 3.0,\n",
        "        'min_child_weight': 5, 'gamma': 0.5\n",
        "    }\n",
        "}\n",
        "with open(f'{MODELS_DIR}/validation_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2, default=str)\n",
        "print(f'‚úÖ M√©tricas guardadas')\n",
        "\n",
        "# Guardar gr√°ficos\n",
        "for img in ['feature_importance.png', 'validation_by_season.png',\n",
        "            'calibration_plot.png', 'feature_selection.png']:\n",
        "    if os.path.exists(img):\n",
        "        shutil.copy(img, f'{OUTPUTS_DIR}/{img}')\n",
        "        print(f'‚úÖ {img} ‚Üí outputs/')\n",
        "\n",
        "print(f'\\nüìÅ Estructura en Drive:')\n",
        "for root, dirs, files in os.walk(PROJECT_DIR):\n",
        "    level = root.replace(PROJECT_DIR, '').count(os.sep)\n",
        "    indent = '  ' * level\n",
        "    print(f'{indent}üìÅ {os.path.basename(root)}/')\n",
        "    for file in sorted(files):\n",
        "        print(f'{\"  \" * (level + 1)}üìÑ {file}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ‚úÖ Resumen: Model Calibration completado\n",
        "\n",
        "### Lo que hicimos:\n",
        "\n",
        "**1. Feature Selection** ‚Äî Descubrimos que menos features = mejor generalizaci√≥n.\n",
        "   Con ~150 series, demasiadas variables generan overfitting.\n",
        "\n",
        "**2. XGBoost optimizado** ‚Äî Hiperpar√°metros agresivamente regularizados\n",
        "   para un dataset peque√±o (max_depth=2, gamma=0.5, fuerte L1/L2).\n",
        "\n",
        "**3. Validaci√≥n LOSO** ‚Äî Predice playoffs futuros sin haber visto los datos.\n",
        "\n",
        "**4. Calibraci√≥n** ‚Äî Las probabilidades son confiables para Monte Carlo.\n",
        "   Para la simulaci√≥n, importa m√°s la calibraci√≥n que la accuracy binaria.\n",
        "\n",
        "**5. Backtest** ‚Äî Verificamos que el campe√≥n real aparece en las\n",
        "   posiciones altas del ranking probabil√≠stico.\n",
        "\n",
        "### üí° Insight para el video:\n",
        "Los playoffs NBA son inherentemente impredecibles ‚Äî incluso con datos\n",
        "y ML, superar al benchmark de \"siempre gana el favorito\" es dif√≠cil.\n",
        "**Eso es exactamente el punto narrativo:** no predecimos UN campe√≥n,\n",
        "mapeamos el espacio probabil√≠stico para revelar qu√© tan competitiva es la liga.\n",
        "\n",
        "### ‚û°Ô∏è Siguiente notebook: `04_simulation_and_viz.ipynb`\n",
        "Donde corremos la simulaci√≥n Monte Carlo completa con los 16 equipos actuales\n",
        "y generamos las visualizaciones para el video. üèÜ"
      ]
    }
  ]
}
