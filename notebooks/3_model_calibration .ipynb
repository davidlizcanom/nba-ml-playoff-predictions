{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NBA Playoffs Simulator — Model Calibration\n",
        "\n",
        "Notebook 03. Acá entreno el XGBoost, valido que generalice bien con Leave-One-Season-Out, y verifico que las probabilidades estén calibradas. Para la simulacion Monte Carlo lo que importa no es tanto la accuracy sino que cuando el modelo diga \"65%\" realmente signifique 65%.\n",
        "\n",
        "También hago feature selection porque con ~150 series de entrenamiento, meter muchos features es receta para overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install xgboost --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import LeaveOneGroupOut, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, brier_score_loss, log_loss, roc_auc_score\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "plt.style.use('dark_background')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_DIR = '/content/drive/MyDrive/nba-playoffs-simulator'\n",
        "DATA_DIR = f'{PROJECT_DIR}/data'\n",
        "\n",
        "df_training = pd.read_csv(f'{DATA_DIR}/training_matchups.csv')\n",
        "df_profiles = pd.read_csv(f'{DATA_DIR}/team_profiles_2026.csv')\n",
        "\n",
        "with open(f'{DATA_DIR}/feature_columns.txt', 'r') as f:\n",
        "    ALL_FEATURE_COLS = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "HISTORICAL_SEASONS = [\n",
        "    '2015-16', '2016-17', '2017-18', '2018-19', '2019-20',\n",
        "    '2020-21', '2021-22', '2022-23', '2023-24', '2024-25'\n",
        "]\n",
        "\n",
        "print(f'Training: {df_training.shape}')\n",
        "print(f'Profiles: {df_profiles.shape}')\n",
        "print(f'Features ({len(ALL_FEATURE_COLS)}): {ALL_FEATURE_COLS}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature selection\n",
        "\n",
        "Con ~150 series, usar 14 features es demasiado — el modelo memoriza ruido. Regla practica: con N muestras no uses mas de N/10 a N/20 features. Eso me da entre 5 y 8.\n",
        "\n",
        "Voy a rankear por correlacion + mutual information y probar distintas cantidades para encontrar el punto optimo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "available_features = [c for c in ALL_FEATURE_COLS if c in df_training.columns]\n",
        "\n",
        "X_all = df_training[available_features].fillna(0)\n",
        "y = df_training['team_a_won'].copy()\n",
        "groups = df_training['season'].copy()\n",
        "\n",
        "print(f'{X_all.shape[0]} series x {X_all.shape[1]} features')\n",
        "print(f'Balance: {y.mean():.1%} favorito gana')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ranking por correlacion + mutual information\n",
        "correlations = X_all.corrwith(y).abs().sort_values(ascending=False)\n",
        "\n",
        "mi_scores = mutual_info_classif(X_all, y, random_state=42)\n",
        "mi_series = pd.Series(mi_scores, index=available_features).sort_values(ascending=False)\n",
        "\n",
        "ranking = pd.DataFrame({\n",
        "    'correlation': correlations,\n",
        "    'mutual_info': mi_series\n",
        "})\n",
        "\n",
        "for col in ['correlation', 'mutual_info']:\n",
        "    ranking[f'{col}_norm'] = ranking[col] / ranking[col].max()\n",
        "ranking['combined_score'] = (\n",
        "    0.5 * ranking['correlation_norm'] + 0.5 * ranking['mutual_info_norm']\n",
        ")\n",
        "ranking = ranking.sort_values('combined_score', ascending=False)\n",
        "\n",
        "for feat, row in ranking.iterrows():\n",
        "    bar = '█' * int(row['combined_score'] * 30)\n",
        "    print(f'{feat:<24} Corr: {row[\"correlation\"]:.3f}  '\n",
        "          f'MI: {row[\"mutual_info\"]:.3f}  '\n",
        "          f'Score: {row[\"combined_score\"]:.3f}  {bar}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pruebo con 2 a 10 features y valido con LOSO\n",
        "\n",
        "logo = LeaveOneGroupOut()\n",
        "baseline_acc = y.mean()\n",
        "\n",
        "print(f'Baseline (siempre favorito): {baseline_acc:.3f}\\n')\n",
        "\n",
        "ranked_features = ranking.index.tolist()\n",
        "results_by_n = []\n",
        "\n",
        "for n_feat in range(2, min(len(ranked_features), 10) + 1):\n",
        "    selected = ranked_features[:n_feat]\n",
        "    X_sel = X_all[selected]\n",
        "\n",
        "    oof_preds = np.zeros(len(y))\n",
        "    oof_probs = np.zeros(len(y))\n",
        "\n",
        "    for train_idx, test_idx in logo.split(X_sel, y, groups):\n",
        "        temp_model = XGBClassifier(\n",
        "            n_estimators=50, max_depth=2, learning_rate=0.05,\n",
        "            subsample=0.7, colsample_bytree=0.8,\n",
        "            reg_alpha=2.0, reg_lambda=3.0, min_child_weight=5,\n",
        "            gamma=0.5, objective='binary:logistic',\n",
        "            eval_metric='logloss', random_state=42,\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "        temp_model.fit(X_sel.iloc[train_idx], y.iloc[train_idx])\n",
        "        oof_preds[test_idx] = temp_model.predict(X_sel.iloc[test_idx])\n",
        "        oof_probs[test_idx] = temp_model.predict_proba(X_sel.iloc[test_idx])[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y, oof_preds)\n",
        "    brier = brier_score_loss(y, oof_probs)\n",
        "    auc = roc_auc_score(y, oof_probs)\n",
        "\n",
        "    results_by_n.append({\n",
        "        'n_features': n_feat,\n",
        "        'features': selected,\n",
        "        'accuracy': acc,\n",
        "        'brier': brier,\n",
        "        'auc': auc,\n",
        "        'vs_baseline': acc - baseline_acc\n",
        "    })\n",
        "\n",
        "    marker = '+' if acc > baseline_acc else ' '\n",
        "    print(f'  {marker} {n_feat} features -> Acc: {acc:.3f} ({acc - baseline_acc:+.3f})  '\n",
        "          f'Brier: {brier:.4f}  AUC: {auc:.3f}')\n",
        "\n",
        "df_feat_search = pd.DataFrame(results_by_n)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.plot(df_feat_search['n_features'], df_feat_search['accuracy'],\n",
        "         'o-', color='#64B5F6', linewidth=2, markersize=8)\n",
        "ax1.axhline(y=baseline_acc, color='#FF5252', linestyle='--',\n",
        "            linewidth=2, label=f'Baseline ({baseline_acc:.0%})')\n",
        "ax1.set_xlabel('Numero de features', fontsize=12)\n",
        "ax1.set_ylabel('Accuracy (LOSO)', fontsize=12)\n",
        "ax1.set_title('Accuracy vs Features', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.plot(df_feat_search['n_features'], df_feat_search['brier'],\n",
        "         's-', color='#FFB74D', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Numero de features', fontsize=12)\n",
        "ax2.set_ylabel('Brier Score (menor = mejor)', fontsize=12)\n",
        "ax2.set_title('Calibracion vs Features', fontsize=13, fontweight='bold')\n",
        "\n",
        "ax3 = axes[2]\n",
        "ax3.plot(df_feat_search['n_features'], df_feat_search['auc'],\n",
        "         'D-', color='#81C784', linewidth=2, markersize=8)\n",
        "ax3.axhline(y=0.5, color='#FF5252', linestyle='--',\n",
        "            linewidth=1, alpha=0.5, label='Random (0.5)')\n",
        "ax3.set_xlabel('Numero de features', fontsize=12)\n",
        "ax3.set_ylabel('ROC AUC', fontsize=12)\n",
        "ax3.set_title('Discriminacion vs Features', fontsize=13, fontweight='bold')\n",
        "ax3.legend(fontsize=10)\n",
        "\n",
        "plt.suptitle('Cuantos features necesita el modelo?',\n",
        "             fontsize=15, fontweight='bold', y=1.03)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_selection.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Elijo la config con mejor Brier entre las que superan el baseline\n",
        "# Si ninguna supera el baseline en accuracy, priorizo calibracion\n",
        "# porque para Monte Carlo eso es lo que importa\n",
        "\n",
        "beats_baseline = df_feat_search[df_feat_search['accuracy'] >= baseline_acc]\n",
        "\n",
        "if len(beats_baseline) > 0:\n",
        "    best_row = beats_baseline.loc[beats_baseline['brier'].idxmin()]\n",
        "    print('Encontre configs que superan el baseline.\\n')\n",
        "else:\n",
        "    best_row = df_feat_search.loc[df_feat_search['brier'].idxmin()]\n",
        "    print('Ninguna config supera el baseline en accuracy pura.')\n",
        "    print('Pero para Monte Carlo lo que importa es la calibracion (Brier),\\n'\n",
        "          'no la prediccion binaria.\\n')\n",
        "\n",
        "FEATURE_COLS = best_row['features']\n",
        "N_BEST = int(best_row['n_features'])\n",
        "\n",
        "print(f'Mejor config: {N_BEST} features')\n",
        "print(f'  Accuracy: {best_row[\"accuracy\"]:.3f} (baseline: {baseline_acc:.3f})')\n",
        "print(f'  Brier:    {best_row[\"brier\"]:.4f}')\n",
        "print(f'  AUC:      {best_row[\"auc\"]:.3f}')\n",
        "print(f'\\nFeatures seleccionados:')\n",
        "for i, feat in enumerate(FEATURE_COLS, 1):\n",
        "    print(f'  {i}. {feat}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrenar XGBoost\n",
        "\n",
        "Hiperparametros bastante conservadores para un dataset chico: arboles superficiales (max_depth=2), learning rate bajo (0.05), regularizacion fuerte (alpha=2, lambda=3). La idea es que no memorice sino que aprenda patrones generales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = X_all[FEATURE_COLS].copy()\n",
        "\n",
        "model = XGBClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=2,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.7,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=2.0,\n",
        "    reg_lambda=3.0,\n",
        "    min_child_weight=5,\n",
        "    gamma=0.5,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    random_state=42,\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "# Training performance (solo como referencia, no es lo que importa)\n",
        "y_pred_train = model.predict(X)\n",
        "y_prob_train = model.predict_proba(X)[:, 1]\n",
        "\n",
        "print(f'Training (referencia):')\n",
        "print(f'  Accuracy:    {accuracy_score(y, y_pred_train):.3f}')\n",
        "print(f'  Brier Score: {brier_score_loss(y, y_prob_train):.4f}')\n",
        "print(f'  ROC AUC:     {roc_auc_score(y, y_prob_train):.3f}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Feature importance\n",
        "importance = pd.DataFrame({\n",
        "    'feature': FEATURE_COLS,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, max(5, len(FEATURE_COLS) * 0.6)))\n",
        "colors = plt.cm.YlOrRd(np.linspace(0.3, 1, len(importance)))\n",
        "ax.barh(importance['feature'], importance['importance'], color=colors)\n",
        "ax.set_xlabel('Importancia', fontsize=12)\n",
        "ax.set_title('Que features importan mas para ganar una serie de playoffs?',\n",
        "             fontsize=14, fontweight='bold', pad=15)\n",
        "\n",
        "top_feat = importance.iloc[-1]\n",
        "ax.annotate(f'El mas predictivo',\n",
        "            xy=(top_feat['importance'], top_feat['feature']),\n",
        "            xytext=(top_feat['importance'] * 0.6, len(importance) - 1.5),\n",
        "            fontsize=11, color='#FFD700', fontweight='bold',\n",
        "            arrowprops=dict(arrowstyle='->', color='#FFD700', lw=1.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()\n",
        "\n",
        "for _, row in importance.iloc[::-1].iterrows():\n",
        "    bar = '█' * int(row['importance'] * 50)\n",
        "    print(f'{row[\"feature\"]:<24} {row[\"importance\"]:.3f}  {bar}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validacion LOSO\n",
        "\n",
        "Leave-One-Season-Out: entreno con 9 temporadas, predigo la que queda afuera. Repito para cada una. Es la forma mas honesta de validar porque simula el escenario real de predecir playoffs futuros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "logo = LeaveOneGroupOut()\n",
        "\n",
        "oof_predictions = np.zeros(len(y))\n",
        "oof_probabilities = np.zeros(len(y))\n",
        "season_results = []\n",
        "\n",
        "for train_idx, test_idx in logo.split(X, y, groups):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    test_season = groups.iloc[test_idx].values[0]\n",
        "\n",
        "    temp_model = XGBClassifier(\n",
        "        n_estimators=50, max_depth=2, learning_rate=0.05,\n",
        "        subsample=0.7, colsample_bytree=0.8,\n",
        "        reg_alpha=2.0, reg_lambda=3.0, min_child_weight=5,\n",
        "        gamma=0.5, objective='binary:logistic',\n",
        "        eval_metric='logloss', random_state=42,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    temp_model.fit(X_train, y_train)\n",
        "\n",
        "    preds = temp_model.predict(X_test)\n",
        "    probs = temp_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    oof_predictions[test_idx] = preds\n",
        "    oof_probabilities[test_idx] = probs\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    n_series = len(y_test)\n",
        "    correct = int((preds == y_test.values).sum())\n",
        "\n",
        "    season_results.append({\n",
        "        'season': test_season,\n",
        "        'n_series': n_series,\n",
        "        'correct': correct,\n",
        "        'accuracy': acc\n",
        "    })\n",
        "\n",
        "    print(f'{test_season}: {correct}/{n_series} ({acc:.0%})')\n",
        "\n",
        "oof_acc = accuracy_score(y, oof_predictions)\n",
        "oof_brier = brier_score_loss(y, oof_probabilities)\n",
        "oof_auc = roc_auc_score(y, oof_probabilities)\n",
        "\n",
        "print(f'\\nGlobal (out-of-fold):')\n",
        "print(f'  Accuracy:    {oof_acc:.3f}  (baseline: {baseline_acc:.3f})')\n",
        "print(f'  Brier Score: {oof_brier:.4f}')\n",
        "print(f'  ROC AUC:     {oof_auc:.3f}')\n",
        "\n",
        "if oof_acc < baseline_acc:\n",
        "    diff = baseline_acc - oof_acc\n",
        "    print(f'\\nQueda {diff:.3f} debajo del baseline en accuracy, pero para Monte Carlo')\n",
        "    print(f'importa mas la calibracion. Un Brier de {oof_brier:.4f} es bueno.')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Resultados por temporada\n",
        "df_season_results = pd.DataFrame(season_results)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "colors_bars = ['#00E676' if acc >= baseline_acc else '#64B5F6'\n",
        "               for acc in df_season_results['accuracy']]\n",
        "\n",
        "bars = ax.bar(df_season_results['season'], df_season_results['accuracy'],\n",
        "              color=colors_bars, edgecolor='white', linewidth=0.5)\n",
        "\n",
        "ax.axhline(y=baseline_acc, color='#FF5252', linestyle='--', linewidth=2,\n",
        "           label=f'Baseline: siempre favorito ({baseline_acc:.0%})')\n",
        "ax.axhline(y=oof_acc, color='#FFD700', linestyle='--', linewidth=2,\n",
        "           label=f'Modelo promedio ({oof_acc:.0%})')\n",
        "\n",
        "for bar, row in zip(bars, df_season_results.itertuples()):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "            f'{row.correct}/{row.n_series}',\n",
        "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "ax.set_ylim(0, 1.15)\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_title('Validacion Leave-One-Season-Out',\n",
        "             fontsize=13, fontweight='bold', pad=15)\n",
        "ax.legend(fontsize=10, loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('validation_by_season.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibracion de probabilidades\n",
        "\n",
        "Para Monte Carlo necesito que las probabilidades sean confiables. Si el modelo dice 60% para OKC, tiene que significar que en situaciones similares OKC gana ~60% de las veces. Un modelo que siempre dice \"favorito gana al 100%\" tiene buena accuracy pero probabilidades inutiles para simular.\n",
        "\n",
        "El Brier Score mide exactamente eso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_bins = 5\n",
        "prob_true, prob_pred = calibration_curve(\n",
        "    y, oof_probabilities, n_bins=n_bins, strategy='uniform'\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.plot([0, 1], [0, 1], 'w--', linewidth=1, alpha=0.5, label='Calibracion perfecta')\n",
        "ax1.plot(prob_pred, prob_true, 's-', color='#64B5F6', linewidth=2,\n",
        "         markersize=10, label='XGBoost')\n",
        "\n",
        "x_line = np.linspace(0, 1, 100)\n",
        "ax1.fill_between(x_line, x_line - 0.15, x_line + 0.15,\n",
        "                 alpha=0.1, color='#00E676', label='Zona aceptable (+/-15%)')\n",
        "\n",
        "ax1.set_xlabel('Probabilidad predicha', fontsize=12)\n",
        "ax1.set_ylabel('Frecuencia real de victoria', fontsize=12)\n",
        "ax1.set_title('Calibration Plot', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.set_xlim(0, 1)\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.hist(oof_probabilities[y == 1], bins=12, alpha=0.7,\n",
        "         label='Favorito gano', color='#00E676')\n",
        "ax2.hist(oof_probabilities[y == 0], bins=12, alpha=0.7,\n",
        "         label='Favorito perdio', color='#FF5252')\n",
        "ax2.set_xlabel('Probabilidad predicha', fontsize=12)\n",
        "ax2.set_ylabel('Frecuencia', fontsize=12)\n",
        "ax2.set_title('Distribucion de probabilidades', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('calibration_plot.png', dpi=150, bbox_inches='tight',\n",
        "            facecolor='black')\n",
        "plt.show()\n",
        "\n",
        "print(f'Brier Score: {oof_brier:.4f}')\n",
        "print(f'  < 0.25: aceptable | < 0.20: bueno | < 0.15: muy bueno')\n",
        "print(f'\\nRango de probabilidades: {oof_probabilities.min():.3f} - {oof_probabilities.max():.3f}')\n",
        "print(f'Std: {oof_probabilities.std():.3f}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backtest historico\n",
        "\n",
        "Simulo los playoffs de las ultimas 3 temporadas para ver si el modelo produce rankings razonables. El criterio: que el campeon real aparezca en el top 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Funciones de simulacion (las mismas que uso en NB04)\n",
        "\n",
        "def simulate_series(prob_a_wins, n_games=7, rng=None):\n",
        "    \"\"\"Simula una serie Bo7. Retorna True si Team A gana.\"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    wins_a, wins_b = 0, 0\n",
        "    games_to_win = (n_games // 2) + 1\n",
        "\n",
        "    # Formato 2-2-1-1-1\n",
        "    home_a_games = {1, 2, 5, 7}\n",
        "    home_boost = 0.03\n",
        "\n",
        "    game_num = 0\n",
        "    while wins_a < games_to_win and wins_b < games_to_win:\n",
        "        game_num += 1\n",
        "        p = prob_a_wins + (home_boost if game_num in home_a_games else -home_boost)\n",
        "        p = np.clip(p, 0.05, 0.95)\n",
        "\n",
        "        if rng.random() < p:\n",
        "            wins_a += 1\n",
        "        else:\n",
        "            wins_b += 1\n",
        "\n",
        "    return wins_a >= games_to_win\n",
        "\n",
        "\n",
        "def get_matchup_probability(team_a_stats, team_b_stats, feature_cols, model):\n",
        "    \"\"\"Probabilidad de que Team A gane la serie.\"\"\"\n",
        "    row = {}\n",
        "    for feat in feature_cols:\n",
        "        base_feat = feat.replace('_diff', '')\n",
        "        if base_feat in team_a_stats.index and base_feat in team_b_stats.index:\n",
        "            val_a = team_a_stats[base_feat]\n",
        "            val_b = team_b_stats[base_feat]\n",
        "            if pd.notna(val_a) and pd.notna(val_b):\n",
        "                if base_feat in ['DEF_RATING', 'TM_TOV_PCT']:\n",
        "                    row[feat] = val_b - val_a\n",
        "                else:\n",
        "                    row[feat] = val_a - val_b\n",
        "            else:\n",
        "                row[feat] = 0\n",
        "        elif feat == 'seed_diff':\n",
        "            seed_a = team_a_stats.get('SEED', team_a_stats.get('PlayoffRank', 4))\n",
        "            seed_b = team_b_stats.get('SEED', team_b_stats.get('PlayoffRank', 4))\n",
        "            row[feat] = seed_b - seed_a\n",
        "        else:\n",
        "            row[feat] = 0\n",
        "\n",
        "    X_matchup = pd.DataFrame([row])[feature_cols]\n",
        "    prob = model.predict_proba(X_matchup)[0][1]\n",
        "    return prob\n",
        "\n",
        "\n",
        "def simulate_playoffs(teams_east, teams_west, feature_cols, model,\n",
        "                      n_simulations=10000, seed=42):\n",
        "    \"\"\"Simula el bracket completo de playoffs n veces.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    r1_matchups = [(0, 7), (3, 4), (2, 5), (1, 6)]\n",
        "    prob_cache = {}\n",
        "\n",
        "    def get_prob(team_a, team_b):\n",
        "        key = (team_a['TEAM_ID'], team_b['TEAM_ID'])\n",
        "        if key not in prob_cache:\n",
        "            prob_cache[key] = get_matchup_probability(\n",
        "                team_a, team_b, feature_cols, model\n",
        "            )\n",
        "        return prob_cache[key]\n",
        "\n",
        "    east_list = [teams_east.iloc[i] for i in range(len(teams_east))]\n",
        "    west_list = [teams_west.iloc[i] for i in range(len(teams_west))]\n",
        "\n",
        "    results = {team['TEAM_NAME']: {\n",
        "        'champion': 0, 'finals': 0, 'conf_finals': 0, 'conf_semis': 0\n",
        "    } for team in east_list + west_list}\n",
        "\n",
        "    finals_matchups = []\n",
        "\n",
        "    for sim in range(n_simulations):\n",
        "        conf_winners = {}\n",
        "\n",
        "        for conf_name, teams in [('East', east_list), ('West', west_list)]:\n",
        "            r1_winners = []\n",
        "            for seed_a, seed_b in r1_matchups:\n",
        "                prob = get_prob(teams[seed_a], teams[seed_b])\n",
        "                a_wins = simulate_series(prob, rng=rng)\n",
        "                winner = teams[seed_a] if a_wins else teams[seed_b]\n",
        "                r1_winners.append(winner)\n",
        "\n",
        "            for w in r1_winners:\n",
        "                results[w['TEAM_NAME']]['conf_semis'] += 1\n",
        "\n",
        "            r2_winners = []\n",
        "            for i in range(0, 4, 2):\n",
        "                a, b = r1_winners[i], r1_winners[i+1]\n",
        "                seed_a = a.get('SEED', a.get('PlayoffRank', 4))\n",
        "                seed_b = b.get('SEED', b.get('PlayoffRank', 4))\n",
        "                if seed_a <= seed_b:\n",
        "                    prob = get_prob(a, b)\n",
        "                    a_wins = simulate_series(prob, rng=rng)\n",
        "                    winner = a if a_wins else b\n",
        "                else:\n",
        "                    prob = get_prob(b, a)\n",
        "                    a_wins = simulate_series(prob, rng=rng)\n",
        "                    winner = b if a_wins else a\n",
        "                r2_winners.append(winner)\n",
        "\n",
        "            for w in r2_winners:\n",
        "                results[w['TEAM_NAME']]['conf_finals'] += 1\n",
        "\n",
        "            a, b = r2_winners[0], r2_winners[1]\n",
        "            seed_a = a.get('SEED', a.get('PlayoffRank', 4))\n",
        "            seed_b = b.get('SEED', b.get('PlayoffRank', 4))\n",
        "            if seed_a <= seed_b:\n",
        "                prob = get_prob(a, b)\n",
        "                a_wins = simulate_series(prob, rng=rng)\n",
        "                conf_winner = a if a_wins else b\n",
        "            else:\n",
        "                prob = get_prob(b, a)\n",
        "                a_wins = simulate_series(prob, rng=rng)\n",
        "                conf_winner = b if a_wins else a\n",
        "\n",
        "            conf_winners[conf_name] = conf_winner\n",
        "\n",
        "        east_champ = conf_winners['East']\n",
        "        west_champ = conf_winners['West']\n",
        "\n",
        "        results[east_champ['TEAM_NAME']]['finals'] += 1\n",
        "        results[west_champ['TEAM_NAME']]['finals'] += 1\n",
        "\n",
        "        finals_matchups.append((east_champ['TEAM_NAME'], west_champ['TEAM_NAME']))\n",
        "\n",
        "        if east_champ.get('NET_RATING', 0) >= west_champ.get('NET_RATING', 0):\n",
        "            prob = get_prob(east_champ, west_champ)\n",
        "            a_wins = simulate_series(prob, rng=rng)\n",
        "            champion = east_champ if a_wins else west_champ\n",
        "        else:\n",
        "            prob = get_prob(west_champ, east_champ)\n",
        "            a_wins = simulate_series(prob, rng=rng)\n",
        "            champion = west_champ if a_wins else east_champ\n",
        "\n",
        "        results[champion['TEAM_NAME']]['champion'] += 1\n",
        "\n",
        "    df_results = pd.DataFrame(results).T\n",
        "    for col in ['champion', 'finals', 'conf_finals', 'conf_semis']:\n",
        "        df_results[f'{col}_pct'] = (df_results[col] / n_simulations * 100).round(2)\n",
        "\n",
        "    df_results = df_results.sort_values('champion', ascending=False)\n",
        "    return df_results, finals_matchups"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "BACKTEST_SEASONS = ['2022-23', '2023-24', '2024-25']\n",
        "KNOWN_CHAMPIONS = {\n",
        "    '2022-23': 'Denver Nuggets',\n",
        "    '2023-24': 'Boston Celtics',\n",
        "    '2024-25': None\n",
        "}\n",
        "\n",
        "# Buscar campeon 2024-25 si esta en los datos\n",
        "finals_2025 = df_training[\n",
        "    (df_training['season'] == '2024-25') & (df_training['round'] == 4)\n",
        "]\n",
        "if len(finals_2025) > 0:\n",
        "    row = finals_2025.iloc[0]\n",
        "    KNOWN_CHAMPIONS['2024-25'] = (\n",
        "        row['team_a_abbr'] if row['team_a_won'] == 1 else row['team_b_abbr']\n",
        "    )\n",
        "\n",
        "print('Campeones para backtest:')\n",
        "for s, c in KNOWN_CHAMPIONS.items():\n",
        "    print(f'  {s}: {c or \"(no encontrado)\"}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_hist_stats = pd.read_csv(f'{DATA_DIR}/historical_team_stats.csv')\n",
        "df_hist_standings = pd.read_csv(f'{DATA_DIR}/historical_standings.csv')\n",
        "\n",
        "for season in BACKTEST_SEASONS:\n",
        "    print(f'--- {season} ---')\n",
        "\n",
        "    # Entreno sin esta temporada\n",
        "    mask_train = df_training['season'] != season\n",
        "    X_bt = df_training.loc[mask_train, FEATURE_COLS]\n",
        "    y_bt = df_training.loc[mask_train, 'team_a_won']\n",
        "\n",
        "    bt_model = XGBClassifier(\n",
        "        n_estimators=50, max_depth=2, learning_rate=0.05,\n",
        "        subsample=0.7, colsample_bytree=0.8,\n",
        "        reg_alpha=2.0, reg_lambda=3.0, min_child_weight=5,\n",
        "        gamma=0.5, objective='binary:logistic',\n",
        "        eval_metric='logloss', random_state=42,\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    bt_model.fit(X_bt, y_bt)\n",
        "\n",
        "    season_stats = df_hist_stats[df_hist_stats['SEASON'] == season].copy()\n",
        "    season_standings = df_hist_standings[df_hist_standings['SEASON'] == season].copy()\n",
        "\n",
        "    if season_stats.empty:\n",
        "        print(f'  Sin stats para {season}\\n')\n",
        "        continue\n",
        "\n",
        "    if 'TeamID' in season_standings.columns:\n",
        "        stand_merge = season_standings[['TeamID', 'Conference', 'PlayoffRank']].rename(\n",
        "            columns={'TeamID': 'TEAM_ID'}\n",
        "        )\n",
        "        season_stats = season_stats.merge(stand_merge, on='TEAM_ID', how='left')\n",
        "\n",
        "    if 'Conference' not in season_stats.columns or 'PlayoffRank' not in season_stats.columns:\n",
        "        print(f'  Sin Conference/Seed para {season}\\n')\n",
        "        continue\n",
        "\n",
        "    season_stats['SEED'] = season_stats['PlayoffRank'].astype(float)\n",
        "    east = season_stats[season_stats['Conference'] == 'East'].nsmallest(8, 'SEED')\n",
        "    west = season_stats[season_stats['Conference'] == 'West'].nsmallest(8, 'SEED')\n",
        "\n",
        "    if len(east) < 8 or len(west) < 8:\n",
        "        print(f'  Equipos insuficientes\\n')\n",
        "        continue\n",
        "\n",
        "    bt_results, _ = simulate_playoffs(\n",
        "        east, west, FEATURE_COLS, bt_model,\n",
        "        n_simulations=5000, seed=42\n",
        "    )\n",
        "\n",
        "    top5 = bt_results.head(5)\n",
        "    champion = KNOWN_CHAMPIONS.get(season, '?')\n",
        "\n",
        "    print(f'  Top 5:')\n",
        "    for rank, (team, row) in enumerate(top5.iterrows(), 1):\n",
        "        marker = ' <-- campeon' if champion and champion in team else ''\n",
        "        print(f'    {rank}. {team:<28} {row[\"champion_pct\"]:>6.1f}%{marker}')\n",
        "\n",
        "    if champion:\n",
        "        for pos, (team, _) in enumerate(bt_results.iterrows(), 1):\n",
        "            if champion in team:\n",
        "                print(f'  {champion} quedo en posicion #{pos}')\n",
        "                break\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guardar modelo y config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "import json\n",
        "\n",
        "MODELS_DIR = f'{PROJECT_DIR}/models'\n",
        "OUTPUTS_DIR = f'{PROJECT_DIR}/outputs'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUTS_DIR, exist_ok=True)\n",
        "\n",
        "# Modelo\n",
        "model_path = f'{MODELS_DIR}/xgb_playoff_model.pkl'\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "print(f'Modelo: {model_path}')\n",
        "\n",
        "# Features seleccionados\n",
        "features_path = f'{MODELS_DIR}/feature_columns.txt'\n",
        "with open(features_path, 'w') as f:\n",
        "    f.write('\\n'.join(FEATURE_COLS))\n",
        "print(f'Features ({len(FEATURE_COLS)}): {features_path}')\n",
        "\n",
        "# Metricas\n",
        "metrics = {\n",
        "    'oof_accuracy': round(oof_acc, 4),\n",
        "    'oof_brier_score': round(oof_brier, 4),\n",
        "    'oof_roc_auc': round(oof_auc, 4),\n",
        "    'baseline_accuracy': round(baseline_acc, 4),\n",
        "    'n_training_samples': int(len(y)),\n",
        "    'n_features': len(FEATURE_COLS),\n",
        "    'features_used': list(FEATURE_COLS),\n",
        "    'model_params': {\n",
        "        'n_estimators': 50, 'max_depth': 2, 'learning_rate': 0.05,\n",
        "        'subsample': 0.7, 'reg_alpha': 2.0, 'reg_lambda': 3.0,\n",
        "        'min_child_weight': 5, 'gamma': 0.5\n",
        "    }\n",
        "}\n",
        "with open(f'{MODELS_DIR}/validation_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2, default=str)\n",
        "print(f'Metricas: validation_metrics.json')\n",
        "\n",
        "# Graficos a outputs\n",
        "for img in ['feature_importance.png', 'validation_by_season.png',\n",
        "            'calibration_plot.png', 'feature_selection.png']:\n",
        "    if os.path.exists(img):\n",
        "        shutil.copy(img, f'{OUTPUTS_DIR}/{img}')\n",
        "        print(f'{img} -> outputs/')\n",
        "\n",
        "print(f'\\nTodo en: {PROJECT_DIR}')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Modelo listo. Lo que queda en Drive:\n",
        "- `models/xgb_playoff_model.pkl` — el modelo entrenado\n",
        "- `models/feature_columns.txt` — los features que usa\n",
        "- `models/validation_metrics.json` — metricas de validacion\n",
        "- `outputs/` — graficos de feature importance, validacion, calibracion\n",
        "\n",
        "El NB04 carga el modelo y corre la simulacion Monte Carlo con los 16 equipos actuales."
      ]
    }
  ]
}
